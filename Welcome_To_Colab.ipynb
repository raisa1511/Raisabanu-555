{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raisa1511/Raisabanu-555/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 1) Install dependencies\n",
        "# -------------------------\n",
        "!pip install --quiet tensorflow==2.16.1 scikit-learn matplotlib pandas numpy shap tqdm\n",
        "\n",
        "# -------------------------\n",
        "# 2) Imports and seeds\n",
        "# -------------------------\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import shap\n",
        "import json\n",
        "import random\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "OUTDIR = \"/content/forecast_outputs\"\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# 3) Synthetic dataset generator\n",
        "# -------------------------\n",
        "def generate_multivariate_series(n_points=5000, n_features=3, noise_std=0.2, seasonal_period=50):\n",
        "    \"\"\"\n",
        "    Generate a synthetic multivariate time series with correlated features + target.\n",
        "    - n_points: time steps\n",
        "    - n_features: number of exogenous features (>=3)\n",
        "    Returns a DataFrame with columns: feat_0 .. feat_{n_features-1}, target\n",
        "    \"\"\"\n",
        "    t = np.arange(n_points)\n",
        "    # base trend and common seasonality\n",
        "    trend = 0.0005 * t  # gentle trend\n",
        "    shared_season = 0.8 * np.sin(2 * np.pi * t / seasonal_period)\n",
        "    data = {}\n",
        "    for i in range(n_features):\n",
        "        # each feature has own amplitude, phase, and noise plus shared signal\n",
        "        amp = 0.5 + 0.5 * (i / (n_features - 1 + 1e-9))\n",
        "        phase = i * 0.5\n",
        "        individual_season = amp * np.sin(2 * np.pi * t / (seasonal_period * (1 + 0.1*i)) + phase)\n",
        "        autoreg = 0.2 * np.roll(np.sin(0.02 * t + i), 1)  # weak auto-like pattern\n",
        "        noise = np.random.normal(scale=noise_std, size=n_points)\n",
        "        data[f\"feat_{i}\"] = trend + shared_season * (0.6 + 0.1*i) + individual_season + autoreg + noise\n",
        "\n",
        "    # Build target as a correlated combination + its own seasonality + extra noise\n",
        "    target = 0.4 * data[\"feat_0\"] + 0.3 * data[\"feat_1\"] + 0.2 * data[\"feat_2\"]\n",
        "    target += 0.5 * np.sin(2 * np.pi * t / (seasonal_period/2 + 1))  # different season for target\n",
        "    target += np.random.normal(scale=noise_std * 1.2, size=n_points)\n",
        "    df = pd.DataFrame(data)\n",
        "    df[\"target\"] = target\n",
        "    return df\n",
        "\n",
        "# Generate data\n",
        "df = generate_multivariate_series(n_points=5000, n_features=4, noise_std=0.25, seasonal_period=60)\n",
        "df.to_csv(os.path.join(OUTDIR, \"synthetic_multivariate.csv\"), index=False)\n",
        "print(\"Generated data shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# Quick plot (saved)\n",
        "plt.figure(figsize=(12,5))\n",
        "for c in df.columns[:4]:\n",
        "    plt.plot(df[c], alpha=0.6, label=c)\n",
        "plt.plot(df[\"target\"], color='k', linewidth=1, label=\"target\")\n",
        "plt.legend()\n",
        "plt.title(\"Synthetic multivariate series (first 4 features + target)\")\n",
        "plt.savefig(os.path.join(OUTDIR, \"series_preview.png\"))\n",
        "plt.close()\n",
        "\n",
        "# -------------------------\n",
        "# 4) Preprocessing and windowing\n",
        "# -------------------------\n",
        "def make_windows(df, input_len=60, horizon=10, target_col=\"target\"):\n",
        "    \"\"\"\n",
        "    Create numpy arrays (X, y) for supervised learning.\n",
        "    X shape: (samples, input_len, n_features)\n",
        "    y shape: (samples, horizon)  # target forecasts only (can expand to multivariate targets)\n",
        "    \"\"\"\n",
        "    features = [c for c in df.columns if c != target_col]\n",
        "    Xs, ys = [], []\n",
        "    arr_feats = df[features].values\n",
        "    arr_target = df[target_col].values\n",
        "    n = len(df)\n",
        "    for start in range(0, n - input_len - horizon + 1):\n",
        "        X = arr_feats[start:start+input_len]\n",
        "        y = arr_target[start+input_len:start+input_len+horizon]\n",
        "        Xs.append(X)\n",
        "        ys.append(y)\n",
        "    Xs = np.array(Xs)\n",
        "    ys = np.array(ys)\n",
        "    return Xs, ys, features\n",
        "\n",
        "INPUT_LEN = 60\n",
        "HORIZON = 10\n",
        "X, y, feature_names = make_windows(df, input_len=INPUT_LEN, horizon=HORIZON)\n",
        "print(\"Windowed shapes:\", X.shape, y.shape)\n",
        "\n",
        "# Split: train / val / test\n",
        "n_samples = X.shape[0]\n",
        "train_end = int(n_samples * 0.7)\n",
        "val_end = int(n_samples * 0.85)\n",
        "X_train, y_train = X[:train_end], y[:train_end]\n",
        "X_val, y_val = X[train_end:val_end], y[train_end:val_end]\n",
        "X_test, y_test = X[val_end:], y[val_end:]\n",
        "\n",
        "# Scale features (fit on train)\n",
        "feat_scaler = StandardScaler()\n",
        "# reshape to 2D to fit\n",
        "ft_train_2d = X_train.reshape(-1, X_train.shape[-1])\n",
        "feat_scaler.fit(ft_train_2d)\n",
        "# apply\n",
        "def scale_X(X, scaler):\n",
        "    s = X.reshape(-1, X.shape[-1])\n",
        "    s = scaler.transform(s)\n",
        "    return s.reshape(X.shape)\n",
        "X_train_s = scale_X(X_train, feat_scaler)\n",
        "X_val_s = scale_X(X_val, feat_scaler)\n",
        "X_test_s = scale_X(X_test, feat_scaler)\n",
        "\n",
        "# scale target\n",
        "t_scaler = StandardScaler()\n",
        "t_scaler.fit(y_train)  # shape: (samples, horizon) works fine\n",
        "y_train_s = t_scaler.transform(y_train)\n",
        "y_val_s = t_scaler.transform(y_val)\n",
        "y_test_s = t_scaler.transform(y_test)\n",
        "\n",
        "# Save scalers\n",
        "import joblib\n",
        "joblib.dump(feat_scaler, os.path.join(OUTDIR, \"feat_scaler.joblib\"))\n",
        "joblib.dump(t_scaler, os.path.join(OUTDIR, \"target_scaler.joblib\"))\n",
        "\n",
        "# -------------------------\n",
        "# 5) Build model function\n",
        "# -------------------------\n",
        "def build_lstm_model(input_len, n_features, horizon, n_units=64, n_layers=2, dropout=0.2, lr=1e-3):\n",
        "    \"\"\"\n",
        "    Build a stacked LSTM many-to-many model that outputs 'horizon' predictions.\n",
        "    \"\"\"\n",
        "    inp = layers.Input(shape=(input_len, n_features), name=\"input_sequence\")\n",
        "    x = inp\n",
        "    for i in range(n_layers - 1):\n",
        "        x = layers.LSTM(n_units, return_sequences=True, name=f\"lstm_{i+1}\")(x)\n",
        "        x = layers.Dropout(dropout)(x)\n",
        "    # Final LSTM returns last output\n",
        "    x = layers.LSTM(n_units, return_sequences=False, name=f\"lstm_{n_layers}\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    out = layers.Dense(horizon, name=\"forecast_dense\")(x)\n",
        "    model = models.Model(inp, out)\n",
        "    model.compile(optimizer=optimizers.Adam(learning_rate=lr), loss=\"mse\", metrics=[\"mae\"])\n",
        "    return model\n",
        "\n",
        "# Quick model summary\n",
        "model = build_lstm_model(INPUT_LEN, X.shape[-1], HORIZON)\n",
        "model.summary()\n",
        "\n",
        "# -------------------------\n",
        "# 6) Hyperparameter search (simple)\n",
        "# -------------------------\n",
        "def run_hp_search(hp_space, max_models=6, epochs=20):\n",
        "    \"\"\"\n",
        "    Try combinations from hp_space (list of dicts). Train each, record val loss.\n",
        "    Return a list of dicts with results and top-k models saved.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for i in range(len(hp_space)):\n",
        "        if i >= max_models:\n",
        "            break\n",
        "        hp = hp_space[i]\n",
        "        print(f\"\\n=== HP trial {i+1}/{min(max_models, len(hp_space))}: {hp}\")\n",
        "        m = build_lstm_model(INPUT_LEN, X.shape[-1], HORIZON,\n",
        "                             n_units=hp[\"n_units\"], n_layers=hp[\"n_layers\"],\n",
        "                             dropout=hp[\"dropout\"], lr=hp[\"lr\"])\n",
        "        cb = [callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True)]\n",
        "        hist = m.fit(X_train_s, y_train_s, validation_data=(X_val_s, y_val_s),\n",
        "                     epochs=epochs, batch_size=hp[\"batch_size\"], callbacks=cb, verbose=0)\n",
        "        best_val_loss = min(hist.history[\"val_loss\"])\n",
        "        print(f\"Best val_loss: {best_val_loss:.6f}\")\n",
        "        # Save model\n",
        "        model_path = os.path.join(OUTDIR, f\"model_hp_{i+1}.keras\") # Changed to .keras\n",
        "        m.save(model_path)\n",
        "        results.append({\"hp\": hp, \"val_loss\": float(best_val_loss), \"model_path\": model_path})\n",
        "    # sort by val_loss\n",
        "    results = sorted(results, key=lambda x: x[\"val_loss\"])\n",
        "    # save results to json\n",
        "    with open(os.path.join(OUTDIR, \"hp_results.json\"), \"w\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    return results\n",
        "\n",
        "# Define a compact hp space to try (you can expand)\n",
        "hp_space = [\n",
        "    {\"n_units\": 32, \"n_layers\": 1, \"dropout\": 0.1, \"lr\": 1e-3, \"batch_size\": 64},\n",
        "    {\"n_units\": 64, \"n_layers\": 2, \"dropout\": 0.2, \"lr\": 5e-4, \"batch_size\": 64},\n",
        "    {\"n_units\": 128, \"n_layers\": 2, \"dropout\": 0.3, \"lr\": 1e-3, \"batch_size\": 32},\n",
        "    {\"n_units\": 64, \"n_layers\": 3, \"dropout\": 0.2, \"lr\": 1e-4, \"batch_size\": 32},\n",
        "    {\"n_units\": 32, \"n_layers\": 2, \"dropout\": 0.15, \"lr\": 3e-4, \"batch_size\": 128},\n",
        "    {\"n_units\": 128, \"n_layers\": 1, \"dropout\": 0.25, \"lr\": 5e-4, \"batch_size\": 64}\n",
        "]\n",
        "results = run_hp_search(hp_space, max_models=6, epochs=25)\n",
        "print(\"\\nTop 5 HP results (val_loss asc):\")\n",
        "for r in results[:5]:\n",
        "    print(r)\n",
        "\n",
        "# Save top-5 hyperparameters in human-readable form\n",
        "with open(os.path.join(OUTDIR, \"top5_hyperparameters.txt\"), \"w\") as f:\n",
        "    for i, r in enumerate(results[:5]):\n",
        "        f.write(f\"Rank {i+1}: val_loss={r['val_loss']}\\nHP: {r['hp']}\\nModel: {r['model_path']}\\n\\n\")\n",
        "\n",
        "# -------------------------\n",
        "# 7) Evaluate best model on test set\n",
        "# -------------------------\n",
        "best = results[0]\n",
        "# Explicitly pass custom objects for standard metrics/losses if there's a deserialization issue\n",
        "custom_objects = {\"mse\": tf.keras.losses.MeanSquaredError(), \"mae\": tf.keras.metrics.MeanAbsoluteError()}\n",
        "best_model = tf.keras.models.load_model(best[\"model_path\"], custom_objects=custom_objects)\n",
        "# Predictions (scaled)\n",
        "y_pred_test_s = best_model.predict(X_test_s)\n",
        "# inverse scale\n",
        "y_pred_test = t_scaler.inverse_transform(y_pred_test_s)\n",
        "y_test_orig = y_test  # already original\n",
        "\n",
        "# Compute RMSE and MAE per horizon and averaged\n",
        "rmse_per_h = np.sqrt(np.mean((y_pred_test - y_test_orig)**2, axis=0))\n",
        "mae_per_h = np.mean(np.abs(y_pred_test - y_test_orig), axis=0)\n",
        "rmse_avg = np.mean(rmse_per_h)\n",
        "mae_avg = np.mean(mae_per_h)\n",
        "metrics = {\"rmse_per_horizon\": rmse_per_h.tolist(), \"mae_per_horizon\": mae_per_h.tolist(),\n",
        "           \"rmse_avg\": float(rmse_avg), \"mae_avg\": float(mae_avg)}\n",
        "print(\"Test metrics:\", metrics)\n",
        "with open(os.path.join(OUTDIR, \"test_metrics.json\"), \"w\") as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "# Plot predictions for a few test windows and save\n",
        "n_plot = 5\n",
        "for i in range(n_plot):\n",
        "    plt.figure(figsize=(8,3))\n",
        "    plt.plot(y_test_orig[i], label=\"true\")\n",
        "    plt.plot(y_pred_test[i], label=\"pred\")\n",
        "    plt.title(f\"Test sample {i} (horizon={HORIZON})\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTDIR, f\"test_pred_{i}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "# -------------------------\n",
        "# 8) Explainability with SHAP\n",
        "# -------------------------\n",
        "def run_shap_explain(model, X_background, X_to_explain, feature_names, horizon_index=0):\n",
        "    \"\"\"\n",
        "    Compute SHAP values explaining the model's output at a particular horizon index.\n",
        "    - model: Keras model that maps (batch, input_len, n_features) -> horizon outputs\n",
        "    - X_background: small background dataset (shape: (k, input_len, n_features))\n",
        "    - X_to_explain: points to explain\n",
        "    Returns shap_values array for features across input timesteps (same shape as X_to_explain).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        explainer = shap.GradientExplainer((model.input, model.output[:, horizon_index]), X_background)\n",
        "        shap_values = explainer.shap_values(X_to_explain)\n",
        "        # shap_values shape for GradientExplainer: list-like; convert appropriately\n",
        "        return shap_values\n",
        "    except Exception as e:\n",
        "        print(\"GradientExplainer failed, falling back to KernelExplainer:\", e)\n",
        "        # Use KernelExplainer on a reduced representation: flatten input\n",
        "        model_flat = lambda x: model.predict(x.reshape((-1, INPUT_LEN, X.shape[-1])))[:, horizon_index]\n",
        "        # Flatten background and to-explain\n",
        "        bg = X_background.reshape(X_background.shape[0], -1)\n",
        "        tx = X_to_explain.reshape(X_to_explain.shape[0], -1)\n",
        "        expl = shap.KernelExplainer(model_flat, bg)\n",
        "        shap_vals = expl.shap_values(tx, nsamples=100)\n",
        "        # reshape back to (samples, input_len, n_features)\n",
        "        shap_vals = np.array(shap_vals).reshape(tx.shape[0], INPUT_LEN, X.shape[-1])\n",
        "        return shap_vals\n",
        "\n",
        "# Choose a small background set from train\n",
        "bg_idx = np.random.choice(len(X_train_s), size=100, replace=False)\n",
        "X_bg = X_train_s[bg_idx]\n",
        "# Points to explain: first 10 test samples\n",
        "X_explain = X_test_s[:10]\n",
        "# For a particular forecast horizon (e.g., horizon_index=0 => immediately next)\n",
        "h_idx = 0\n",
        "shap_vals = run_shap_explain(best_model, X_bg, X_explain, feature_names, horizon_index=h_idx)\n",
        "\n",
        "# shap_vals may be in different formats; normalize into array shape (samples, input_len, n_features)\n",
        "if isinstance(shap_vals, list):\n",
        "    # shap.GradientExplainer often returns an array-like inside a list\n",
        "    shap_arr = np.array(shap_vals[0])\n",
        "else:\n",
        "    shap_arr = np.array(shap_vals)\n",
        "print(\"SHAP array shape:\", shap_arr.shape)\n",
        "\n",
        "# Aggregate importance across timesteps for each feature (mean absolute)\n",
        "feature_imp = np.mean(np.abs(shap_arr), axis=(0,1)) if shap_arr.ndim==3 else np.mean(np.abs(shap_arr))\n",
        "# if array is (samples, input_len, n_feat) reduce sample & time dims\n",
        "if shap_arr.ndim == 3:\n",
        "    feat_imp_per_feature = np.mean(np.abs(shap_arr), axis=(0,1))  # shape (n_features,)\n",
        "else:\n",
        "    feat_imp_per_feature = feature_imp\n",
        "\n",
        "# Create bar plot\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(feature_names, feat_imp_per_feature)\n",
        "plt.title(f\"Mean |SHAP| per feature (horizon_index={h_idx})\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTDIR, f\"shap_feature_importance_h{h_idx}.png\"))\n",
        "plt.close()\n",
        "\n",
        "# Save shap raw values for later analysis\n",
        "np.save(os.path.join(OUTDIR, f\"shap_values_h{h_idx}.npy\"), shap_arr)\n",
        "\n",
        "# -------------------------\n",
        "# 9) Summary report (text)\n",
        "# -------------------------\n",
        "report = {\n",
        "    \"project\": \"Advanced Time Series Forecasting — synthetic dataset + LSTM\",\n",
        "    \"dataset\": {\n",
        "        \"n_points\": int(len(df)),\n",
        "        \"n_features\": int(X.shape[-1]),\n",
        "        \"input_len\": INPUT_LEN,\n",
        "        \"horizon\": HORIZON,\n",
        "        \"file\": \"synthetic_multivariate.csv\"\n",
        "    },\n",
        "    \"best_hyperparams\": results[0][\"hp\"],\n",
        "    \"test_metrics\": metrics,\n",
        "    \"saved_outputs_dir\": OUTDIR,\n",
        "    \"plots\": [\n",
        "        \"series_preview.png\",\n",
        "        *[f\"test_pred_{i}.png\" for i in range(n_plot)],\n",
        "        f\"shap_feature_importance_h{h_idx}.png\"\n",
        "    ]\n",
        "}\n",
        "with open(os.path.join(OUTDIR, \"summary_report.json\"), \"w\") as f:\n",
        "    json.dump(report, f, indent=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 985
        },
        "id": "Wlzoie2t7IQI",
        "outputId": "d95d2492-2ffd-4e69-ab86-4c249a76d9df"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated data shape: (5000, 5)\n",
            "     feat_0    feat_1    feat_2    feat_3    target\n",
            "0  0.019477  0.300496  0.730121  1.089341  0.348450\n",
            "1  0.068372  0.487759  0.910682  1.095773  0.541459\n",
            "2  0.370675  0.263505  0.933635  1.186954  0.330271\n",
            "3  0.693092  0.736007  1.199127  1.466429  1.197374\n",
            "4  0.354056  1.101616  1.551080  1.092907  0.697695\n",
            "Windowed shapes: (4931, 60, 4) (4931, 10)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_47\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_47\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_sequence (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m4\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m17,664\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_84 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m33,024\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_85 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ forecast_dense (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_sequence (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">17,664</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_84 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_85 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ forecast_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m51,338\u001b[0m (200.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,338</span> (200.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m51,338\u001b[0m (200.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,338</span> (200.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== HP trial 1/6: {'n_units': 32, 'n_layers': 1, 'dropout': 0.1, 'lr': 0.001, 'batch_size': 64}\n",
            "Best val_loss: 0.503963\n",
            "\n",
            "=== HP trial 2/6: {'n_units': 64, 'n_layers': 2, 'dropout': 0.2, 'lr': 0.0005, 'batch_size': 64}\n",
            "Best val_loss: 0.460516\n",
            "\n",
            "=== HP trial 3/6: {'n_units': 128, 'n_layers': 2, 'dropout': 0.3, 'lr': 0.001, 'batch_size': 32}\n",
            "Best val_loss: 0.424864\n",
            "\n",
            "=== HP trial 4/6: {'n_units': 64, 'n_layers': 3, 'dropout': 0.2, 'lr': 0.0001, 'batch_size': 32}\n",
            "Best val_loss: 0.433126\n",
            "\n",
            "=== HP trial 5/6: {'n_units': 32, 'n_layers': 2, 'dropout': 0.15, 'lr': 0.0003, 'batch_size': 128}\n",
            "Best val_loss: 0.500349\n",
            "\n",
            "=== HP trial 6/6: {'n_units': 128, 'n_layers': 1, 'dropout': 0.25, 'lr': 0.0005, 'batch_size': 64}\n",
            "Best val_loss: 0.439606\n",
            "\n",
            "Top 5 HP results (val_loss asc):\n",
            "{'hp': {'n_units': 128, 'n_layers': 2, 'dropout': 0.3, 'lr': 0.001, 'batch_size': 32}, 'val_loss': 0.42486366629600525, 'model_path': '/content/forecast_outputs/model_hp_3.keras'}\n",
            "{'hp': {'n_units': 64, 'n_layers': 3, 'dropout': 0.2, 'lr': 0.0001, 'batch_size': 32}, 'val_loss': 0.43312641978263855, 'model_path': '/content/forecast_outputs/model_hp_4.keras'}\n",
            "{'hp': {'n_units': 128, 'n_layers': 1, 'dropout': 0.25, 'lr': 0.0005, 'batch_size': 64}, 'val_loss': 0.43960633873939514, 'model_path': '/content/forecast_outputs/model_hp_6.keras'}\n",
            "{'hp': {'n_units': 64, 'n_layers': 2, 'dropout': 0.2, 'lr': 0.0005, 'batch_size': 64}, 'val_loss': 0.4605162739753723, 'model_path': '/content/forecast_outputs/model_hp_2.keras'}\n",
            "{'hp': {'n_units': 32, 'n_layers': 2, 'dropout': 0.15, 'lr': 0.0003, 'batch_size': 128}, 'val_loss': 0.5003489851951599, 'model_path': '/content/forecast_outputs/model_hp_5.keras'}\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step\n",
            "Test metrics: {'rmse_per_horizon': [0.5412021963408656, 0.5593662375801661, 0.533902550890319, 0.5592954035591355, 0.5439355323584166, 0.5319616838004425, 0.5364406222445536, 0.5752259249009631, 0.5658997845262911, 0.5613121539603255], 'mae_per_horizon': [0.4332302188212115, 0.44343419201007667, 0.427660941906963, 0.4462046113180993, 0.4306334483845214, 0.4220728086820219, 0.42621109623113274, 0.45781426608330655, 0.45337463289187735, 0.4476821397320008], 'rmse_avg': 0.5508542090161479, 'mae_avg': 0.43883183560612116}\n",
            "SHAP array shape: (10, 60, 4, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "107f2124",
        "outputId": "1969cb31-3bd6-42c0-9616-a9d4aec11026"
      },
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Assuming OUTDIR is defined from the previous cells\n",
        "results_path = os.path.join(OUTDIR, \"hp_results.json\")\n",
        "\n",
        "if os.path.exists(results_path):\n",
        "    with open(results_path, \"r\") as f:\n",
        "        all_results = json.load(f)\n",
        "    print(\"Full Hyperparameter Search Results (sorted by val_loss):\")\n",
        "    for i, res in enumerate(all_results):\n",
        "        print(f\"Rank {i+1}: Val Loss = {res['val_loss']:.6f}, HP = {res['hp']}\")\n",
        "else:\n",
        "    print(f\"Error: hp_results.json not found at {results_path}\")\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Hyperparameter Search Results (sorted by val_loss):\n",
            "Rank 1: Val Loss = 0.424864, HP = {'n_units': 128, 'n_layers': 2, 'dropout': 0.3, 'lr': 0.001, 'batch_size': 32}\n",
            "Rank 2: Val Loss = 0.433126, HP = {'n_units': 64, 'n_layers': 3, 'dropout': 0.2, 'lr': 0.0001, 'batch_size': 32}\n",
            "Rank 3: Val Loss = 0.439606, HP = {'n_units': 128, 'n_layers': 1, 'dropout': 0.25, 'lr': 0.0005, 'batch_size': 64}\n",
            "Rank 4: Val Loss = 0.460516, HP = {'n_units': 64, 'n_layers': 2, 'dropout': 0.2, 'lr': 0.0005, 'batch_size': 64}\n",
            "Rank 5: Val Loss = 0.500349, HP = {'n_units': 32, 'n_layers': 2, 'dropout': 0.15, 'lr': 0.0003, 'batch_size': 128}\n",
            "Rank 6: Val Loss = 0.503963, HP = {'n_units': 32, 'n_layers': 1, 'dropout': 0.1, 'lr': 0.001, 'batch_size': 64}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pyf_VbV2Hsqd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}